{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting the raw json data obtained from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Updated on 20th December 2023\n",
    "@author: Dragos Gruia\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from base64 import b64encode\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been downloaded via the API, specify in order the following information: the website linked to the data, the name of the file containing the data, whether or not speech was downloaded, and the output directory for your formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_domain = 'ic3v2.cognitron.co.uk' #Website from which we want to download the data\n",
    "output_file = 'patientsv2-23-11-23-cog.obj' #Progress report file from cognitron\n",
    "download_speech = False # If true, only speech files will download\n",
    "output_dir = \"./data_temp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import in-house functions used for parsing and formatting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and perform parsing\n",
    "\n",
    "path_to_file = output_file.replace('.obj','.json')\n",
    "df = load_json(path_to_file)\n",
    "dfdata = extract_from_data(df, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove subjects with missing information\n",
    "# and reset the index.\n",
    "\n",
    "dfdata.dropna(subset=['taskID'], inplace=True) \n",
    "dfdata = dfdata.reset_index(drop = True) \n",
    "\n",
    "# Harmonise the data across clinical tests\n",
    "dfdata = task_specific_cleaning(dfdata)   \n",
    "\n",
    "#Create summary metrics for each test \n",
    "dfdatascores = separate_score(dfdata, \"Scores\")\n",
    "dfdata = dfdata.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create files for each task containing summaries of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process summary task data\n",
    "# Output each task as a separate file\n",
    "\n",
    "summarydfs = []\n",
    "for task in np.unique(dfdatascores.taskID):\n",
    "    dfexamp = dfdatascores[dfdatascores[\"taskID\"] == task]\n",
    "    dfexamp = dfexamp.dropna(axis = 1)\n",
    "    finddf = pd.merge(dfexamp.dropna(axis = 1), dfdata.drop([\"Rawdata\", \"Scores\"], axis = 1), on = [\"user_id\", \"taskID\"])\n",
    "    finddf.to_csv(f\"{output_dir}/{task}.csv\")\n",
    "    summarydfs.append(finddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process summary questionnaire data\n",
    "# Output each questionnaire as a separate file\n",
    "\n",
    "questions = [task for task in dfdata.taskID if task.startswith(\"q\")]\n",
    "unique_questions = list(np.unique(questions))\n",
    "summarydfs = []\n",
    "\n",
    "for task in unique_questions:\n",
    "    if task in questions:\n",
    "        dfexamp = dfdata[dfdata[\"taskID\"] == task].reset_index(drop = True)\n",
    "        dfexamp_resp = separate_response_obj(dfexamp, col_response =\"RespObject\" )\n",
    "        dfexamp_resp.to_csv(f\"{output_dir}/{task}_questionnaire.csv\")\n",
    "        summarydfs.append(dfexamp_resp)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create files for each task containing raw trial by trial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the format of the clinical data changes after a specific timepoint, we separate the data into two dataframes, and any further formatting is done separately to each one. The two dataframes are merged at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata1=dfdata[dfdata.startTime.astype(int) <= 1645639554944]\n",
    "dfdata2=dfdata[dfdata.startTime.astype(int) > 1645639554944]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data is then pre-processed separately for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws1 = rawdata(dfdata1.reset_index(drop=True))\n",
    "raws2 = rawdata(dfdata2.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatted raw data is then merged from the two dataframes, and outputted separately for each task and questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdfs = []\n",
    "for task in tqdm(np.unique(dfdata.taskID)):\n",
    "    dfs_task = []\n",
    "    for df in raws1:  \n",
    "        try:\n",
    "            if df.shape[0] != 0:\n",
    "                if np.unique(df[\"taskID\"]).item() == task:\n",
    "                    dfs_task.append(df)\n",
    "        except:\n",
    "            print(task)\n",
    "            print(df)\n",
    "            break \n",
    "    print(task)\n",
    "    for df in raws2:  \n",
    "        try:\n",
    "            if df.shape[0] != 0:\n",
    "                if np.unique(df[\"taskID\"]).item() == task:\n",
    "                    dfs_task.append(df)\n",
    "        except:\n",
    "            print(task)\n",
    "            print(df)\n",
    "            break \n",
    "    print(task)\n",
    "        \n",
    "    dff = pd.concat(dfs_task)\n",
    "    dff.to_csv(f\"{output_dir}/{task}_raw3.csv\")\n",
    "    rawdfs.append(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF data contains speech, decrypt it, save as .WAV files and annotate each speech file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionanry of all possible trials in the speech tasks, to be used for annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_stimuli = {\n",
    "    \"IC3_Repetition\": ['VILLAGE', #20 words\n",
    "        'MANNER',\n",
    "        'GRAVITY',\n",
    "        'AUDIENCE'\n",
    "        'COFFEE',\n",
    "        'PURPOSE',\n",
    "        'CONCEPT',\n",
    "        'MOMENT',\n",
    "        'TREASON',\n",
    "        'FIRE',\n",
    "        'ELEPHANT',\n",
    "        'CHARACTER',\n",
    "        'BONUS',\n",
    "        'RADIO',\n",
    "        'TRACTOR'\n",
    "        'HOSPITAL',\n",
    "        'FUNNEL',\n",
    "        'EFFORT',\n",
    "        'TRIBUTE',\n",
    "        'STUDENT'],\n",
    "    \"IC3_Reading\": ['if', #11 words\n",
    "        'frilt',\n",
    "        'home',\n",
    "        'to',\n",
    "        'dwelb',\n",
    "        'or',\n",
    "        'listening',\n",
    "        'and',\n",
    "        'concert',\n",
    "        'blosp',\n",
    "        'treasure'],\n",
    "    \"IC3_NamingTest\": ['funnel', #30 pictures\n",
    "        'tree',\n",
    "        'dominos',\n",
    "        'toothbrush',\n",
    "        'boomerang',\n",
    "        'mask',\n",
    "        'snail',\n",
    "        'acorn',\n",
    "        'scroll',\n",
    "        'seahorse',\n",
    "        'raquet',\n",
    "        'unicorn',\n",
    "        'bed',\n",
    "        'scissors',\n",
    "        'harmonica',\n",
    "        'whistle',\n",
    "        'canoe',\n",
    "        'helicopter',\n",
    "        'volcano',\n",
    "        'house',\n",
    "        'harp',\n",
    "        'dart',\n",
    "        'igloo',\n",
    "        'pencil',\n",
    "        'mushroom',\n",
    "        'saw',\n",
    "        'comb',\n",
    "        'bench',\n",
    "        'camel',\n",
    "        'hanger'],\n",
    "     \"IC3_SpokenPicture\": ['0', #2 pictures\n",
    "        '1'\n",
    "    ]     \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrypt and annotate speech files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, stimuli_values in speech_stimuli.items():\n",
    "    \n",
    "    os.chdir(output_speech_files)\n",
    "    \n",
    "    # Open the formatted trial data for each task\n",
    "    \n",
    "    speech_data = pd.read_csv((f\"{output_speech_files}_raw/{task}.csv\"))\n",
    "    trial_data = pd.read_csv((f\"{output_speech_files}_raw/{task}_raw.csv\"))\n",
    "    \n",
    "    \n",
    "    for index, sub in speech_data.iterrows():\n",
    "        \n",
    "        os.chdir(output_speech_files)        \n",
    "        \n",
    "        if bool(sub.empty == False):\n",
    "            \n",
    "            # Create a directory for each subject\n",
    "            \n",
    "            voiceData = sub[\"media\"]\n",
    "            user_id = sub[\"user_id\"]\n",
    "            path2Task = sub[\"taskID\"]\n",
    "            \n",
    "            if os.path.isdir(user_id) == False:\n",
    "                os.mkdir(user_id)\n",
    "            os.chdir(user_id)\n",
    "\n",
    "            if os.path.isdir(path2Task) == False:\n",
    "                os.mkdir(path2Task)\n",
    "            os.chdir(path2Task) \n",
    "            \n",
    "            # Clean the encrypted speech data\n",
    "            \n",
    "            temp_trial_data = trial_data[trial_data.loc[:, \"user_id\"].isin([user_id])] \n",
    "            voiceData = re.split(\"\\'data:audio/wav;base64,\",voiceData)\n",
    "            voiceData.pop(0)\n",
    "            voiceData = list(map(lambda x: x.replace('\\',', ''), voiceData))\n",
    "            \n",
    "            # Add exception for when the subject did not consent to the speech task\\\n",
    "                \n",
    "            if len(temp_trial_data) == 0:\n",
    "                empty_file = open(\"no_speech.txt\",\"w\")\n",
    "                empty_file.write(\"No speech files for this task\")\n",
    "                empty_file.close()\n",
    "                continue\n",
    "            \n",
    "            if task == \"IC3_SpokenPicture\":\n",
    "                temp_trial_data[\"Target\"] = temp_trial_data[\"Level\"].astype(str)\n",
    "            \n",
    "            # Address bug where one or more speech files are missing\n",
    "            \n",
    "            if len(voiceData) > len(temp_trial_data.Target):  \n",
    "                temp_stimuli = pd.Series(stimuli_values)   \n",
    "                if task == \"IC3_Repetition\":\n",
    "                    new_row = pd.DataFrame({'Target': \"Unknown_stimuli\"}, index=[0])\n",
    "                else:\n",
    "                    missing_stimuli = temp_stimuli[(~temp_stimuli.isin(temp_trial_data.Target)).to_list().index(True)].upper()\n",
    "                    new_row = pd.DataFrame({'Target': missing_stimuli}, index=[0])\n",
    "\n",
    "                temp_trial_data = pd.concat([new_row, temp_trial_data.loc[:]]).reset_index(drop=True)\n",
    "                \n",
    "            # Decrypt and output each speech file\n",
    "                \n",
    "            for count,value in enumerate(voiceData):\n",
    "                tempVoice = voiceData[count]   \n",
    "                temp_name = temp_trial_data.Target.iloc[count].upper() + '_' + str(count) + '_' + path2Task + '_' + user_id + '.wav'      \n",
    "                test_wav = open(temp_name,\"wb\")\n",
    "                temp_bin = b64decode(tempVoice)\n",
    "                test_wav.write(temp_bin)\n",
    "                test_wav.close()\n",
    "                \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
