{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preproc_functions_controls import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/dg519/Documents/normative_paper_github/data'\n",
    "remote_data_folders='/data_ic3online_cognition'\n",
    "supervised_data_folders=['/data_healthy_v1','/data_healthy_v2']\n",
    "folder_structure=['/summary_data','/trial_data','/speech']\n",
    "output_clean_folder ='/data_healthy_cleaned'\n",
    "list_of_tasks = ['IC3_NVtrailMaking','IC3_Orientation', 'IC3_PearCancellation', 'IC3_rs_digitSpan', 'IC3_rs_spatialSpan', 'IC3_rs_PAL', 'IC3_rs_SRT', 'IC3_rs_CRT', 'IC3_SemanticJudgment', 'IC3_i4i_IDED', 'IC3_i4i_motorControl','IC3_calculation', 'IC3_GestureRecognition', 'IC3_AuditorySustainedAttention','IC3_BBCrs_blocks', 'IC3_Comprehension','IC3_rs_oddOneOut', 'IC3_TaskRecall']\n",
    "list_of_questionnaires = ['q_IC3_demographics', 'q_IC3_fatigue','q_IC3_GDS','q_IC3_apathy']\n",
    "list_of_speech = ['IC3_NamingTest','IC3_Reading', 'IC3_Repetition']\n",
    "folder_list = ['data_healthy_v1', 'data_healthy_v2', 'data_ic3online_cognition', 'data_healthy_merged']\n",
    "merged_data_folder ='/data_healthy_combined'\n",
    "data_format = '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Cleaning normative data.Creating inclusion criteria...We removed 6 people who failed both Orientation and Pear Cancellation, from all tasks.\n",
      "We removed 407 who indicated they have a neurological disorder, 91 who have a history of dementia and 92 who are younger than 40.\n",
      "We removed 11 people who cheated.\n",
      "We removed 17 people who failed both Orientation and Pear Cancellation, from all tasks.\n",
      "We removed 33 who indicated they have a neurological disorder, and 4 who are younger than 40.\n",
      "Done\n",
      "Merging data across sites...Merged IC3_NVtrailMaking\n",
      "Merged IC3_Orientation\n",
      "Merged IC3_PearCancellation\n",
      "Merged IC3_rs_digitSpan\n",
      "Merged IC3_rs_spatialSpan\n",
      "Merged IC3_rs_PAL\n",
      "Merged IC3_rs_SRT\n",
      "Merged IC3_rs_CRT\n",
      "Merged IC3_SemanticJudgment\n",
      "Merged IC3_i4i_IDED\n",
      "Merged IC3_i4i_motorControl\n",
      "Merged IC3_calculation\n",
      "Merged IC3_GestureRecognition\n",
      "Merged IC3_AuditorySustainedAttention\n",
      "Merged IC3_BBCrs_blocks\n",
      "Merged IC3_Comprehension\n",
      "Merged IC3_rs_oddOneOut\n",
      "Merged IC3_TaskRecall\n",
      "Merged IC3_NVtrailMaking2\n",
      "Merged IC3_NVtrailMaking3\n",
      "Merged IC3_NamingTest\n",
      "Merged IC3_Reading\n",
      "Merged IC3_Repetition\n",
      "18\n",
      "Done\n",
      "Pre-processing IC3_NVtrailMaking...Done\n",
      "Pre-processing IC3_Orientation...Done\n",
      "Pre-processing IC3_PearCancellation...Done\n",
      "Pre-processing IC3_rs_digitSpan...Done\n",
      "Pre-processing IC3_rs_spatialSpan...Done\n",
      "Pre-processing IC3_rs_PAL...Done\n",
      "Pre-processing IC3_rs_SRT...Done\n",
      "Pre-processing IC3_rs_CRT...Done\n",
      "Pre-processing IC3_SemanticJudgment...Done\n",
      "Pre-processing IC3_i4i_IDED...Done\n",
      "Pre-processing IC3_i4i_motorControl...Done\n",
      "Pre-processing IC3_calculation...Done\n",
      "Pre-processing IC3_GestureRecognition...Done\n",
      "Pre-processing IC3_AuditorySustainedAttention...Done\n",
      "Pre-processing IC3_BBCrs_blocks...Done\n",
      "Pre-processing IC3_Comprehension...Done\n",
      "Pre-processing IC3_rs_oddOneOut...Done\n",
      "Pre-processing IC3_TaskRecall...Done\n",
      "Pre-processing q_IC3_demographics...Done\n",
      "Pre-processing q_IC3_fatigue...Questionnaire q_IC3_fatigue does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_GDS...Questionnaire q_IC3_GDS does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_apathy...Questionnaire q_IC3_apathy does not have a specific preprocessing function.\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "main_preprocessing(root_path, list_of_tasks, list_of_questionnaires, list_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dg519/Documents/normative_paper_github/data/data_healthy_combined'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tasks = list_of_tasks[:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'IC3_NVtrailMaking'\n",
    "inclusion_criteria = pd.read_csv('inclusion_criteria.csv')['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6f181486-0e14-450e-bfe3-fb85e925adba\n",
       "1       12e7f6c0-f77e-4064-88fc-abb9e1952693\n",
       "2       54270eee-d9a7-4d44-b72f-f0137372acce\n",
       "3       68732c3e-9d23-4212-ad3c-22acaddff7e0\n",
       "4       4b21c478-5926-4c59-b37f-60d6a40ccf43\n",
       "                        ...                 \n",
       "6707       ic3study30079-sessionMT2-versionB\n",
       "6708       ic3study30065-sessionMT3-versionB\n",
       "6709         ic3study10013-session2-versionB\n",
       "6710         ic3study30047-session1-versionB\n",
       "6711         ic3study10017-session2-versionB\n",
       "Name: user_id, Length: 6712, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inclusion_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_general_outliers(root_path, merged_data_folder, task_name, inclusion_criteria,  data_format, folder_structure=['/summary_data','/trial_data','/speech']):\n",
    "    \n",
    "    path_to_data = root_path + merged_data_folder\n",
    "    print(f'{path_to_data}{folder_structure[0]}/{task_name}{data_format}')\n",
    "    try:\n",
    "        df = pd.read_csv(f'{path_to_data}/{folder_structure[0]}/{task_name}{data_format}', low_memory=False)\n",
    "        df = df[df.user_id.isin(inclusion_criteria)]\n",
    "        \n",
    "        df_raw = pd.read_csv(f'{path_to_data}/{folder_structure[1]}/{task_name}_raw{data_format}', low_memory=False)\n",
    "        df_raw = df_raw[df_raw.user_id.isin(inclusion_criteria)]\n",
    "    except:\n",
    "        print(f'Error in loading {task_name}. File might not exist.')\n",
    "        return None,None\n",
    "\n",
    "    df.drop_duplicates(subset=['user_id'],keep=\"last\", inplace=True)\n",
    "    df.drop(columns=['Unnamed: 0','Level','type','RespObject','sequenceObj', 'dynamicDifficulty'], inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    if ('Unnamed: 0' in df_raw.columns):\n",
    "        \n",
    "        df_raw = df_raw.rename(columns={'Unnamed: 0':'Level_filter'})\n",
    "\n",
    "    df_raw.drop_duplicates(subset=['user_id','Level_filter'],keep=\"last\", inplace=True)\n",
    "    df_raw.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    return df,df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6f181486-0e14-450e-bfe3-fb85e925adba\n",
       "1       12e7f6c0-f77e-4064-88fc-abb9e1952693\n",
       "2       54270eee-d9a7-4d44-b72f-f0137372acce\n",
       "3       68732c3e-9d23-4212-ad3c-22acaddff7e0\n",
       "4       4b21c478-5926-4c59-b37f-60d6a40ccf43\n",
       "                        ...                 \n",
       "6707       ic3study30079-sessionMT2-versionB\n",
       "6708       ic3study30065-sessionMT3-versionB\n",
       "6709         ic3study10013-session2-versionB\n",
       "6710         ic3study30047-session1-versionB\n",
       "6711         ic3study10017-session2-versionB\n",
       "Name: user_id, Length: 6712, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inclusion_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dg519/Documents/normative_paper_github/data/data_healthy_combined/summary_data/IC3_NVtrailMaking.csv\n"
     ]
    }
   ],
   "source": [
    "df,df_raw = remove_general_outliers(root_path, merged_data_folder, task_name, inclusion_criteria,  data_format, folder_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dg519/Documents/normative_paper_github/data/data_healthy_combined/summary_data/IC3_NVtrailMaking2.csv\n",
      "/Users/dg519/Documents/normative_paper_github/data/data_healthy_combined/summary_data/IC3_NVtrailMaking3.csv\n"
     ]
    }
   ],
   "source": [
    "df2,df_raw2 = remove_general_outliers(root_path, merged_data_folder, f'{task_name}2', inclusion_criteria, data_format, folder_structure)\n",
    "df3,df_raw3 = remove_general_outliers(root_path, merged_data_folder, f'{task_name}3', inclusion_criteria, data_format, folder_structure)\n",
    "df,df_raw = trailmaking_preproc(df,df2,df3,df_raw,df_raw2,df_raw3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dg519/Documents/normative_paper_github/data'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata_healthy_cleaned\u001b[m\u001b[m/     \u001b[34mdata_ic3online_cognition\u001b[m\u001b[m/ \u001b[34mdata_patients_v2\u001b[m\u001b[m/\n",
      "\u001b[34mdata_healthy_combined\u001b[m\u001b[m/    \u001b[34mdata_patients_cleaned\u001b[m\u001b[m/    \u001b[34moutput_data\u001b[m\u001b[m/\n",
      "\u001b[34mdata_healthy_v1\u001b[m\u001b[m/          \u001b[34mdata_patients_merged\u001b[m\u001b[m/\n",
      "\u001b[34mdata_healthy_v2\u001b[m\u001b[m/          \u001b[34mdata_patients_v1\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data_healthy_combined'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(merged_data_folder[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tasks = list_of_tasks[:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 6 people who failed both Orientation and Pear Cancellation, from all tasks.\n",
      "We removed 407 who indicated they have a neurological disorder, 91 who have a history of dementia and 92 who are younger than 40.\n",
      "We removed 11 people who cheated.\n"
     ]
    }
   ],
   "source": [
    "ids_remote = general_outlier_detection_remoteSetting(root_path, remote_data_folders, folder_structure, screening_list = ['q_IC3_demographicsHealthy_questionnaire.csv', 'q_IC3_metacog_questionnaire.csv', 'IC3_Orientation.csv', 'IC3_PearCancellation.csv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_outlier_detection_supervisedSetting(root_path, supervised_data_folders, folder_structure, screening_list = ['q_IC3_demographics_questionnaire.csv', 'IC3_Orientation.csv', 'IC3_PearCancellation.csv']): \n",
    "\n",
    "    # Read the data for v1\n",
    "\n",
    "    os.chdir(root_path + supervised_data_folders[0] + folder_structure[0])\n",
    "\n",
    "    df_dem_tp1 = pd.read_csv(screening_list[0], low_memory=False)\n",
    "    df_orient_tp1 = pd.read_csv(screening_list[1], low_memory=False)\n",
    "    df_pear_tp1 = pd.read_csv(screening_list[2], low_memory=False)\n",
    "\n",
    "    # Read the data for v2\n",
    "\n",
    "    os.chdir(root_path + supervised_data_folders[1] + folder_structure[0])\n",
    "\n",
    "    df_dem_tp2 = pd.read_csv(screening_list[0], low_memory=False)\n",
    "    df_orient_tp2 = pd.read_csv(screening_list[1], low_memory=False)\n",
    "    df_pear_tp2 = pd.read_csv(screening_list[2], low_memory=False)\n",
    "\n",
    "    # Concatenate the two timepoints\n",
    "\n",
    "    df_dem = pd.concat([df_dem_tp1, df_dem_tp2], ignore_index=True)\n",
    "    df_orient = pd.concat([df_orient_tp1, df_orient_tp2], ignore_index=True)\n",
    "    df_pear = pd.concat([df_pear_tp1, df_pear_tp2], ignore_index=True)\n",
    "\n",
    "    # Remove duplicates, NAs and reset index\n",
    "\n",
    "    df_dem.drop_duplicates(subset=['user_id'],keep=\"first\", inplace=True)\n",
    "    df_dem.dropna(subset=['user_id'], inplace=True)\n",
    "    df_dem.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_orient.drop_duplicates(subset=['user_id'],keep=\"first\", inplace=True)\n",
    "    df_orient.dropna(subset=['user_id'], inplace=True)\n",
    "    df_orient.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_pear.drop_duplicates(subset=['user_id'],keep=\"first\", inplace=True)\n",
    "    df_pear.dropna(subset=['user_id'], inplace=True)\n",
    "    df_pear.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Find the user_id who are in both df_pear and df_pear\n",
    "\n",
    "    cleaned_ids_supervised = list(set(df_dem['user_id']).intersection(set(df_orient['user_id'])).intersection(set(df_pear['user_id'])))\n",
    "\n",
    "    df_pear = df_pear[df_pear.user_id.isin(cleaned_ids_supervised)]  \n",
    "    df_orient = df_orient[df_orient.user_id.isin(cleaned_ids_supervised)]  \n",
    "    df_dem = df_dem[df_dem.user_id.isin(cleaned_ids_supervised)]  \n",
    "\n",
    "    # Drop the user_id, if they fail the screening test\n",
    "\n",
    "    ids_failed_screen = []\n",
    "    for subs in cleaned_ids_supervised:\n",
    "        if (df_orient[df_orient.user_id == subs].SummaryScore < 3).bool() and (df_pear[df_pear.user_id == subs].SummaryScore <= 0.80).bool():\n",
    "            \n",
    "            ids_failed_screen.append(subs)\n",
    "            df_dem = df_dem.drop(df_dem[df_dem.user_id == subs].index).reset_index(drop=True)\n",
    "            df_orient = df_orient.drop(df_orient[df_orient.user_id == subs].index).reset_index(drop=True)\n",
    "            df_pear = df_pear.drop(df_pear[df_pear.user_id == subs].index).reset_index(drop=True)\n",
    "\n",
    "    print(f'We removed {len(ids_failed_screen)} people who failed both Orientation and Pear Cancellation, from all tasks.')\n",
    "\n",
    "\n",
    "    cleaned_ids_supervised = df_pear.user_id\n",
    "\n",
    "    # Remove people who are not neurologically healthy\n",
    "\n",
    "    to_remove = (df_dem.Q12_R != \"SKIPPED\") | (df_dem.Q14_R != \"SKIPPED\") | (df_dem.Q1_R < 40)    \n",
    "    print(f'We removed {sum((df_dem.Q12_R != \"SKIPPED\") | (df_dem.Q14_R != \"SKIPPED\"))} who indicated they have a neurological disorder, and {sum(df_dem.Q1_R < 40)} who are younger than 40.')\n",
    "\n",
    "    df_dem = df_dem[~to_remove].reset_index(drop=True)\n",
    "    df_pear = df_pear[~to_remove].reset_index(drop=True)\n",
    "    df_orient = df_orient[~to_remove].reset_index(drop=True)\n",
    "\n",
    "    cleaned_ids_supervised = df_pear.user_id\n",
    "\n",
    "    return cleaned_ids_supervised # Return participant ids that passed exclusion criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 17 people who failed both Orientation and Pear Cancellation, from all tasks.\n",
      "We removed 33 who indicated they have a neurological disorder, and 4 who are younger than 40.\n"
     ]
    }
   ],
   "source": [
    "ids_supervised =  general_outlier_detection_supervisedSetting(root_path, supervised_data_folders, folder_structure, screening_list = ['q_IC3_demographics_questionnaire.csv', 'IC3_Orientation.csv', 'IC3_PearCancellation.csv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclusion_criteria = pd.concat([ids_remote, ids_supervised], ignore_index=True)\n",
    "inclusion_criteria.reset_index(drop=True,inplace=True)\n",
    "if not os.path.isdir(merged_data_folder):\n",
    "    os.mkdir(merged_data_folder)\n",
    "inclusion_criteria.to_csv(f'{root_path}{merged_data_folder}/inclusion_criteria.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_control_data_across_sites(root_path, folder_structure, supervised_data_folders, remote_data_folders, list_of_tasks,list_of_questionnaires,list_of_speech, data_format, merged_data_folder):\n",
    "        \n",
    "    os.chdir(root_path)\n",
    "\n",
    "    # Create folder structure\n",
    "    \n",
    "    if os.path.isdir(merged_data_folder[1:]) == False:\n",
    "        os.mkdir(merged_data_folder[1:])\n",
    "    os.chdir(merged_data_folder[1:])\n",
    "\n",
    "    if os.path.isdir(folder_structure[0][1:]) == False:\n",
    "        os.mkdir(folder_structure[0][1:])\n",
    "        \n",
    "    if os.path.isdir(folder_structure[1][1:]) == False:\n",
    "        os.mkdir(folder_structure[1][1:])\n",
    "    \n",
    "    if os.path.isdir(folder_structure[2][1:]) == False:\n",
    "        os.mkdir(folder_structure[2][1:])\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    # Merge data from clinical tests\n",
    "    \n",
    "    for taskName in list_of_tasks:\n",
    "            \n",
    "        \n",
    "        summary_task_path = root_path + supervised_data_folders[0] + folder_structure[0]\n",
    "        raw_task_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "        \n",
    "        df_v1 = pd.read_csv(f'{summary_task_path}/{taskName}{data_format}', low_memory=False)\n",
    "        df_v1_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        summary_task_path = root_path + supervised_data_folders[1] + folder_structure[0]\n",
    "        raw_task_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    "        \n",
    "        # Special case for IDED task that has two versions\n",
    "        \n",
    "        if taskName == \"IC3_i4i_IDED\": \n",
    "            df_v2 = pd.read_csv((f'{summary_task_path}/{taskName}2{data_format}'), low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}2_raw{data_format}'), low_memory=False)\n",
    "        else:   \n",
    "            df_v2 = pd.read_csv(f'{summary_task_path}/{taskName}{data_format}', low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        \n",
    "        summary_task_path = root_path + remote_data_folders + folder_structure[0]\n",
    "        raw_task_path = root_path + remote_data_folders + folder_structure[1]\n",
    "\n",
    "        df_cog = pd.read_csv(f'{summary_task_path}/{taskName}{data_format}', low_memory=False)\n",
    "        df_cog_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        df = pd.concat([df_v1, df_v2, df_cog], ignore_index=True)\n",
    "        df_raw = pd.concat([df_v1_raw, df_v2_raw, df_cog_raw], ignore_index=True)\n",
    "        \n",
    "        output_folder = root_path + merged_data_folder\n",
    "        \n",
    "        df.to_csv(f'{output_folder}/{folder_structure[0]}/{taskName}{data_format}', index=False)\n",
    "        df_raw.to_csv(f'{output_folder}/{folder_structure[1]}/{taskName}_raw{data_format}', index=False)\n",
    "        print(f'Merged {taskName}')\n",
    "        \n",
    "    # Merge data from speech\n",
    "        \n",
    "    for taskName in list_of_speech:\n",
    "        \n",
    "        raw_speech_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "        df_v1_raw = pd.read_csv((f'{raw_speech_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        raw_speech_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    "        df_v2_raw = pd.read_csv((f'{raw_speech_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "\n",
    "        df_raw = pd.concat([df_v1_raw, df_v2_raw], ignore_index=True)\n",
    "        \n",
    "        output_folder = root_path + merged_data_folder\n",
    "        df_raw.to_csv(f'{output_folder}/{folder_structure[1]}/{taskName}_raw.csv', index=False)\n",
    "        print(f'Merged {taskName}')\n",
    "        \n",
    "    # Merge data from questionnaires\n",
    "\n",
    "    for taskName in list_of_questionnaires:\n",
    "        \n",
    "        if taskName == 'q_IC3_demographics':\n",
    "            \n",
    "            summary_task_path = root_path + supervised_data_folders[0] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "            \n",
    "            df_v1 = pd.read_csv((f'{summary_task_path}/{taskName}Healthy_questionnaire.csv'), low_memory=False)\n",
    "            df_v1_2 = pd.read_csv((f'{summary_task_path}/{taskName}_questionnaire.csv'), low_memory=False)\n",
    "            df_v1_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            df_v1_raw_2 = pd.read_csv((f'{raw_task_path}/{taskName}Healthy_raw.csv'), low_memory=False)\n",
    "\n",
    "            summary_task_path = root_path + supervised_data_folders[1] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    " \n",
    "            df_v2 = pd.read_csv((f'{summary_task_path}/{taskName}_questionnaire.csv'), low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            summary_task_path = root_path + remote_data_folders + folder_structure[0]\n",
    "            raw_task_path = root_path + remote_data_folders + folder_structure[1]\n",
    "\n",
    "            df_cog = pd.read_csv((f'{summary_task_path}/{taskName}Healthy_questionnaire.csv'), low_memory=False)\n",
    "            df_cog_raw = pd.read_csv((f'{raw_task_path}/{taskName}Healthy_raw.csv'), low_memory=False)\n",
    "\n",
    "            df = pd.concat([df_v1, df_v1_2, df_v2, df_cog], ignore_index=True)\n",
    "            df_raw = pd.concat([df_v1_raw, df_v1_raw_2, df_v2_raw, df_cog_raw], ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            summary_task_path = root_path + supervised_data_folders[0] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "            \n",
    "            df_v1 = pd.read_csv(f'{summary_task_path}/{taskName}_questionnaire.csv', low_memory=False)\n",
    "            df_v1_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            summary_task_path = root_path + supervised_data_folders[1] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    "            \n",
    "            df_v2 = pd.read_csv(f'{summary_task_path}/{taskName}_questionnaire.csv', low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            summary_task_path = root_path + remote_data_folders + folder_structure[0]\n",
    "            raw_task_path = root_path + remote_data_folders + folder_structure[1]\n",
    "            \n",
    "            df_cog = pd.read_csv(f'{summary_task_path}/{taskName}_questionnaire.csv', low_memory=False)\n",
    "            df_cog_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            df = pd.concat([df_v1, df_v2, df_cog], ignore_index=True)\n",
    "            df_raw = pd.concat([df_v1_raw, df_v2_raw, df_cog_raw], ignore_index=True)\n",
    "        \n",
    "        \n",
    "            \n",
    "        output_folder = root_path + merged_data_folder\n",
    "\n",
    "        df.to_csv(f'{output_folder}/{folder_structure[0]}/{taskName}{data_format}', index=False)\n",
    "        df_raw.to_csv(f'{output_folder}/{folder_structure[1]}/{taskName}_raw.csv', index=False)\n",
    "        print(f'Merged {taskName}')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged IC3_Orientation\n",
      "Merged IC3_PearCancellation\n",
      "Merged IC3_rs_digitSpan\n",
      "Merged IC3_rs_spatialSpan\n",
      "Merged IC3_rs_PAL\n",
      "Merged IC3_rs_SRT\n",
      "Merged IC3_rs_CRT\n",
      "Merged IC3_SemanticJudgment\n",
      "Merged IC3_i4i_IDED\n",
      "Merged IC3_i4i_motorControl\n",
      "Merged IC3_calculation\n",
      "Merged IC3_GestureRecognition\n",
      "Merged IC3_AuditorySustainedAttention\n",
      "Merged IC3_BBCrs_blocks\n",
      "Merged IC3_Comprehension\n",
      "Merged IC3_NVtrailMaking\n",
      "Merged IC3_rs_oddOneOut\n",
      "Merged IC3_TaskRecall\n",
      "Merged IC3_NamingTest\n",
      "Merged IC3_Reading\n",
      "Merged IC3_Repetition\n",
      "Merged q_IC3_demographics\n",
      "Merged q_IC3_fatigue\n",
      "Merged q_IC3_GDS\n",
      "Merged q_IC3_apathy\n"
     ]
    }
   ],
   "source": [
    "merge_control_data_across_sites(root_path, folder_structure, supervised_data_folders, remote_data_folders, list_of_tasks,list_of_questionnaires,list_of_speech, data_format, merged_data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_general_outliers(root_path, merged_data_folder, task_name, inclusion_criteria,  data_format, folder_structure=['/summary_data','/trial_data','/speech']):\n",
    "    \n",
    "    path_to_data = root_path + merged_data_folder\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(f'{path_to_data}/{folder_structure[0]}/{task_name}{data_format}', low_memory=False)\n",
    "        df = df[df.user_id.isin(inclusion_criteria)]\n",
    "\n",
    "        df_raw = pd.read_csv(f'{path_to_data}/{folder_structure[1]}/{task_name}_raw{data_format}', low_memory=False)\n",
    "        df_raw = df_raw[df_raw.user_id.isin(inclusion_criteria)]\n",
    "    except:\n",
    "        print(f'Error in loading {task_name}. File might not exist.')\n",
    "        return None,None\n",
    "\n",
    "    df.drop_duplicates(subset=['user_id'],keep=\"last\", inplace=True)\n",
    "    df.drop(columns=['Unnamed: 0','Level','type','RespObject','sequenceObj', 'dynamicDifficulty'], inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    if ('Unnamed: 0' in df_raw.columns):\n",
    "        \n",
    "        df_raw = df_raw.rename(columns={'Unnamed: 0':'Level_filter'})\n",
    "\n",
    "    df_raw.drop_duplicates(subset=['user_id','Level_filter'],keep=\"last\", inplace=True)\n",
    "    df_raw.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    return df,df_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'IC3_Orientation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df_raw = remove_general_outliers(root_path, merged_data_folder, task_name, inclusion_criteria,  data_format, folder_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_preproc(df,df_raw):\n",
    "\n",
    "    df_raw.loc[df_raw.RT<400,'RT'] = np.nan\n",
    "    df_raw.loc[df_raw.RT>10000,'RT'] = np.nan\n",
    "\n",
    "    scores = [None] * len(df.user_id)\n",
    "    errors = [None] * len(df.user_id)\n",
    "    meanRTs =[None] * len(df.user_id)\n",
    "    medianRTs =[None] * len(df.user_id)\n",
    "    meanErrorRTs =[None] * len(df.user_id)\n",
    "    meanCorrRTs =[None] * len(df.user_id)\n",
    "    medianErrorRTs =[None] * len(df.user_id)\n",
    "    medianCorrRTs =[None] * len(df.user_id)\n",
    "    numNAs = [None] * len(df.user_id)\n",
    "\n",
    "    for count,id in enumerate(df.user_id):\n",
    "        df_raw_temp = df_raw[df_raw.user_id == id]\n",
    "        \n",
    "        date_raw = df_raw_temp.iloc[0].TimeRespEnabled\n",
    "        datestart = datetime.datetime.fromtimestamp(float(date_raw)/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        datestart = datestart.split(' ')[1].split(':')\n",
    "        \n",
    "        if any(x == int(datestart[0]) for x in [6, 12, 18, 0]) and (int(datestart[1]) < 30):\n",
    "            df_raw_temp.loc[df_raw_temp.Level == 4, 'correct'] = True\n",
    "        elif any(x == int(datestart[0]) for x in [5, 11, 17, 23]) and (int(datestart[1]) > 30):\n",
    "            df_raw_temp.loc[df_raw_temp.Level == 4, 'correct'] = True\n",
    "            \n",
    "        errors[count] = (sum(df_raw_temp.correct == False))\n",
    "        scores[count] = sum(df_raw_temp.correct)\n",
    "        \n",
    "        meanRTs[count] = np.nanmean(df_raw_temp.RT)\n",
    "        meanErrorRTs[count] = np.nanmean(df_raw_temp[df_raw_temp.correct == False].RT) if (~df_raw_temp.correct).any() else np.nan\n",
    "        meanCorrRTs[count] = np.nanmean(df_raw_temp[df_raw_temp.correct == True].RT) if (df_raw_temp.correct).any() else np.nan\n",
    "        medianRTs[count] = np.nanmedian(df_raw_temp.RT)\n",
    "        medianErrorRTs[count] = np.nanmedian(df_raw_temp[df_raw_temp.correct == False].RT) if (~df_raw_temp.correct).any() else np.nan\n",
    "        medianCorrRTs[count] = np.nanmedian(df_raw_temp[df_raw_temp.correct == True].RT) if (df_raw_temp.correct).any() else np.nan\n",
    "        numNAs[count] = df_raw_temp.RT.isna().sum() / len(df_raw_temp.RT)\n",
    "        \n",
    "    df_temp = pd.DataFrame({\"user_id\":df.user_id, \"score\":scores, \"errors\":errors, \"meanRT\":meanRTs, \"medianRT\":medianRTs, \"medianErrorRT\":medianErrorRTs, \"medianCorrRT\":medianCorrRTs, \"meanErrorRT\":meanErrorRTs, \"meanCorrRT\":meanCorrRTs, \"numNAs\":numNAs})\n",
    "\n",
    "    df[\"SummaryScore\"] = df_temp.score\n",
    "    df[\"totalCorrect\"] = df_temp.score\n",
    "    df[\"errors\"] = df_temp.errors\n",
    "    df[\"medianRT\"] = df_temp.medianRT\n",
    "    df[\"meanRT\"] = df_temp.meanRT\n",
    "    df[\"medianCorrectRT\"] = df_temp.medianCorrRT\n",
    "    df[\"medianErrorRT\"] = df_temp.medianErrorRT\n",
    "    df[\"meanCorrectRT\"] = df_temp.meanCorrRT\n",
    "    df[\"meanErrorRT\"] = df_temp.meanErrorRT\n",
    "\n",
    "    exc = ((df.timeOffScreen > 10000) | (df.focusLossCount > 2) | (df_temp.numNAs > 0.6) | (df.SummaryScore < 2)) & (df.SummaryScore <4)\n",
    "    df.drop(df[exc].index, inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df_raw = df_raw[df_raw.user_id.isin(df.user_id)]\n",
    "    df_raw.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    return df,df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df_raw = orientation_preproc(df,df_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_preprocessed_data(df,df_raw, root_path, output_clean_folder, folder_structure, clean_file_extension, data_format):\n",
    "    \n",
    "    os.chdir(root_path)\n",
    "    \n",
    "    if not os.path.isdir(output_clean_folder[1:]):\n",
    "        os.mkdir(output_clean_folder[1:])\n",
    "        os.mkdir(f'.{output_clean_folder}{folder_structure[0]}')\n",
    "        os.mkdir(f'.{output_clean_folder}{folder_structure[1]}')\n",
    "\n",
    "    df.to_csv(f\".{output_clean_folder}{folder_structure[0]}/{df.loc[0,'taskID']}{clean_file_extension}{data_format}\", index=False)\n",
    "    df_raw.to_csv(f\".{output_clean_folder}{folder_structure[1]}/{df.loc[0,'taskID']}_raw{clean_file_extension}{data_format}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file_extension='_cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_preprocessed_data(df,df_raw, root_path, output_clean_folder, folder_structure, clean_file_extension, data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographics_preproc(root_path, merged_data_folder, questionnaire_name, inclusion_criteria, folder_structure, data_format, clean_file_extension):\n",
    "    \n",
    "    os.chdir(root_path + merged_data_folder)\n",
    "    \n",
    "    try:\n",
    "        df_dem_summary = pd.read_csv(f'.{folder_structure[0]}/{questionnaire_name}{data_format}', low_memory=False)\n",
    "        df_dem_summary = df_dem_summary[df_dem_summary.user_id.isin(inclusion_criteria)]\n",
    "        \n",
    "        df_dem = pd.read_csv(f'.{folder_structure[1]}/{questionnaire_name}_raw{data_format}', low_memory=False)\n",
    "        df_dem = df_dem[df_dem.user_id.isin(inclusion_criteria)]    \n",
    "\n",
    "    except:\n",
    "        print(f'Error in loading {questionnaire_name}. File might not exist.')\n",
    "        return None\n",
    "    \n",
    "    df_dem.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df_dem =df_dem.drop_duplicates(subset=['user_id','question'], keep='last').reset_index(drop=True)\n",
    "    df_dem_summary =df_dem_summary.drop_duplicates(subset='user_id',keep='last').reset_index(drop=True)\n",
    "    \n",
    "    # Extract demographics of interest\n",
    "    \n",
    "    age = df_dem.groupby(['question']).get_group('<center>Howoldareyou?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    gender = df_dem.groupby(['question']).get_group('<center>Whatisyourbiologicalsex?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    education = df_dem.groupby(['question']).get_group('<center>Whatisyourhighestlevelofeducation?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    device = df_dem.groupby(['question']).get_group('<center>Whatdeviceareyouusingatthemoment?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    english = df_dem.groupby(['question']).get_group('<center>HowwouldyourateyourproficiencyinEnglish?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    depression = df_dem.groupby(['question']).get_group('<center>Areyoucurrentlytakinganymedicationfordepression?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    anxiety = df_dem.groupby(['question']).get_group('<center>Areyoucurrentlytakinganymedicationforanxiety?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    dyslexia = df_dem.groupby(['question']).get_group('<center>DoyouhaveDyslexia,oranyotherproblemswithreadingandwriting?</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "    risks = df_dem.groupby(['question']).get_group('<center>Haveyoueverbeentoldyouhavethefollowing?Tickallthatapplies</center>').loc[:,['response','user_id']].reset_index(drop=True)\n",
    "\n",
    "    # Clean each variable\n",
    "    \n",
    "    age.response = pd.to_numeric(age.response)\n",
    "    age.loc[age.response < 40,'response'] = np.nan\n",
    "    \n",
    "    gender.response = gender.response.replace(['53','55','65','78','71','72'],np.nan)\n",
    "    gender.replace(['Male','Female'],[0,1], inplace=True)\n",
    "\n",
    "    education.response = education.response.replace('1',np.nan)\n",
    "    education.response = education.response.replace('Secondary/HighSchoolDiploma','Secondary/HighSchool-A-levels')\n",
    "    education.response = education.response.replace('Primary/ElementarySchool','SecondarySchool-GCSE')\n",
    "    education.replace(['SecondarySchool-GCSE','Secondary/HighSchool-A-levels','ProfessionalDegree','Bachelor\\'sDegree','Master\\'sDegree','PhD'],[0,1,1,2,3,3], inplace=True)\n",
    "                \n",
    "    device = device.merge(df_dem_summary.loc[:,['user_id','os']], on='user_id', how='outer')\n",
    "    device.response = device.response.fillna(device.os)\n",
    "    device.drop('os',axis=1,inplace=True)\n",
    "    \n",
    "    english.response.replace({'3': 1, '4': 1, 'No': np.nan, '2':1,'1':0}, inplace=True)\n",
    "    depression.response.replace({'No':0, 'Yes':1, 'SKIPPED':0}, inplace=True)   \n",
    "    anxiety.response.replace({'No':0, 'Yes':1, 'SKIPPED':0}, inplace=True)\n",
    "    dyslexia.response.replace({'Yes':1,'No':0,'Tablet':0,'Touchscreen':0}, inplace=True)\n",
    "    \n",
    "    risks.drop_duplicates(keep='last', inplace=True)\n",
    "    risks.replace(np.nan, ' ', inplace=True)\n",
    "    risks.response = risks.response.str.lower()\n",
    "    risks['diabetes'] = risks.response.apply(lambda x: 'diabetes' in x).replace([True,False], [1,0])\n",
    "    risks['highbloodpressure'] = risks.response.apply(lambda x: 'highbloodpressure' in x).replace([True,False], [1,0])\n",
    "    risks['highcholesterol'] = risks.response.apply(lambda x: ('highcholesterol' in x) or ('highcholesterole' in x)).replace([True,False], [1,0])\n",
    "    risks['heartdisease'] = risks.response.apply(lambda x: 'heartdisease' in x).replace([True,False], [1,0])\n",
    "    risks['kidneydisease'] = risks.response.apply(lambda x: 'kidneydisease' in x).replace([True,False], [1,0])\n",
    "    risks['alcoholdependency'] = risks.response.apply(lambda x: 'alcoholdependency' in x).replace([True,False], [1,0])\n",
    "    risks['over-weight'] = risks.response.apply(lambda x: ('over-weight' in x) or ('overweight' in x)).replace([True,False], [1,0])\n",
    "    risks['long-termsmoker'] = risks.response.apply(lambda x: ('long-termsmoker' in x) or ('longtermsmoker' in x)).replace([True,False], [1,0])\n",
    "    risks['ex-smoker'] = risks.response.apply(lambda x: ('ex-smoker' in x) or ('exsmoker' in x)).replace([True,False], [1,0])\n",
    "    risks.loc[(risks['long-termsmoker'] & risks['ex-smoker']).astype(bool),'ex-smoker'] = 0\n",
    "    risks.response = risks.iloc[:,2:].sum(axis=1)\n",
    "    \n",
    "    age.drop_duplicates(subset=\"user_id\",keep='last',inplace=True)\n",
    "    gender.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    education.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    device.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    english.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    depression.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    anxiety.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    dyslexia.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    risks.drop_duplicates(subset=\"user_id\", keep='last',inplace=True)\n",
    "    \n",
    "    age.rename(columns={'response':'age'}, inplace=True)\n",
    "    gender.rename(columns={'response':'gender'}, inplace=True)\n",
    "    education.rename(columns={'response':'education'}, inplace=True)\n",
    "    device.rename(columns={'response':'device'}, inplace=True)\n",
    "    english.rename(columns={'response':'english'}, inplace=True)\n",
    "    depression.rename(columns={'response':'depression'}, inplace=True)\n",
    "    anxiety.rename(columns={'response':'anxiety'}, inplace=True)\n",
    "    dyslexia.rename(columns={'response':'dyslexia'}, inplace=True)\n",
    "    risks.rename(columns={'response':'risks'}, inplace=True)\n",
    "    \n",
    "    age.dropna(inplace=True)\n",
    "    gender.dropna(inplace=True)\n",
    "    education.dropna(inplace=True)\n",
    "    device.dropna(inplace=True)\n",
    "    english.dropna(inplace=True)\n",
    "    depression.dropna(inplace=True)\n",
    "    anxiety.dropna(inplace=True)\n",
    "    dyslexia.dropna(inplace=True)\n",
    "    risks.dropna(inplace=True)\n",
    "              \n",
    "    # Merge and format\n",
    "    \n",
    "    healthy_demographics = age.merge(gender,on='user_id').merge(education,on='user_id').merge(device,on='user_id').merge(english,on='user_id').merge(depression,on='user_id').merge(anxiety,on='user_id').merge(risks,on='user_id').merge(dyslexia,on='user_id')\n",
    "    healthy_demographics.education = healthy_demographics.education.astype(int)\n",
    "    \n",
    "    one_hot_encoded_data = pd.get_dummies(healthy_demographics, columns = ['device', 'education'])\n",
    "    one_hot_encoded_data.rename(columns={'education_1':'education_Alevels', 'education_2':'education_bachelors','education_3':'education_postBachelors'}, inplace=True)\n",
    "    one_hot_encoded_data.rename(columns={'device_1':'device_tablet', 'device_0':'device_phone'}, inplace=True)\n",
    "    one_hot_encoded_data.rename(columns={'english':'english_secondLanguage'}, inplace=True)\n",
    "    one_hot_encoded_data.replace({True:1, False:0}, inplace=True)\n",
    "    one_hot_encoded_data.loc[:,'gender':'education_postBachelors'] = one_hot_encoded_data.loc[:,'gender':'education_postBachelors'] -0.5\n",
    "\n",
    "    # Save \n",
    "    \n",
    "    one_hot_encoded_data.to_csv(f'.{folder_structure[0]}/{questionnaire_name}{clean_file_extension}{data_format}', index=False)\n",
    "    \n",
    "    return one_hot_encoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_name = 'q_IC3_demographics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics = demographics_preproc(root_path, merged_data_folder, questionnaire_name, inclusion_criteria, folder_structure, data_format,clean_file_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preproc_functions_patients import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/dg519/Documents/normative_paper_github/data'\n",
    "list_of_tasks = ['IC3_Orientation', 'IC3_PearCancellation', 'IC3_rs_digitSpan', 'IC3_rs_spatialSpan', 'IC3_rs_PAL', 'IC3_rs_SRT', 'IC3_rs_CRT', 'IC3_SemanticJudgment', 'IC3_i4i_IDED', 'IC3_i4i_motorControl','IC3_calculation', 'IC3_GestureRecognition', 'IC3_AuditorySustainedAttention','IC3_BBCrs_blocks', 'IC3_Comprehension', 'IC3_NVtrailMaking','IC3_rs_oddOneOut', 'IC3_TaskRecall']\n",
    "list_of_questionnaires = ['q_IC3_demographics', 'q_IC3_fatigue','q_IC3_GDS','q_IC3_apathy', 'q_IC3_IADL']\n",
    "list_of_speech = ['IC3_NamingTest','IC3_Reading', 'IC3_Repetition']\n",
    "patient_data_folders=['/data_patients_v1','/data_patients_v2']\n",
    "folder_structure=['/summary_data','/trial_data','/speech']\n",
    "output_clean_folder ='/data_patients_cleaned'\n",
    "merged_data_folder ='/data_patients_merged'\n",
    "clean_file_extension='_cleaned'\n",
    "data_format='.csv'\n",
    "clinical_information = '/Users/dg519/Documents/normative_paper_github/data/output_data/patients_database260924.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Merging data across sites...Merged IC3_Orientation\n",
      "Merged IC3_PearCancellation\n",
      "Merged IC3_rs_digitSpan\n",
      "Merged IC3_rs_spatialSpan\n",
      "Merged IC3_rs_PAL\n",
      "Merged IC3_rs_SRT\n",
      "Merged IC3_rs_CRT\n",
      "Merged IC3_SemanticJudgment\n",
      "Merged IC3_i4i_IDED\n",
      "Merged IC3_i4i_motorControl\n",
      "Merged IC3_calculation\n",
      "Merged IC3_GestureRecognition\n",
      "Merged IC3_AuditorySustainedAttention\n",
      "Merged IC3_BBCrs_blocks\n",
      "Merged IC3_Comprehension\n",
      "Merged IC3_NVtrailMaking\n",
      "Merged IC3_rs_oddOneOut\n",
      "Merged IC3_TaskRecall\n",
      "Merged IC3_NamingTest\n",
      "Merged IC3_Reading\n",
      "Merged IC3_Repetition\n",
      "Merged q_IC3_demographics\n",
      "Merged q_IC3_fatigue\n",
      "Merged q_IC3_GDS\n",
      "Merged q_IC3_apathy\n",
      "Merged q_IC3_IADL\n",
      "Done\n",
      "Pre-processing IC3_Orientation...Done\n",
      "Pre-processing IC3_PearCancellation...Done\n",
      "Pre-processing IC3_rs_digitSpan...Done\n",
      "Pre-processing IC3_rs_spatialSpan...Done\n",
      "Pre-processing IC3_rs_PAL...Done\n",
      "Pre-processing IC3_rs_SRT...Done\n",
      "Pre-processing IC3_rs_CRT...Done\n",
      "Pre-processing IC3_SemanticJudgment...Done\n",
      "Pre-processing IC3_i4i_IDED...Done\n",
      "Pre-processing IC3_i4i_motorControl...Done\n",
      "Pre-processing IC3_calculation...Done\n",
      "Pre-processing IC3_GestureRecognition...Done\n",
      "Pre-processing IC3_AuditorySustainedAttention...Done\n",
      "Pre-processing IC3_BBCrs_blocks...Done\n",
      "Pre-processing IC3_Comprehension...Done\n",
      "Pre-processing IC3_NVtrailMaking...Done\n",
      "Pre-processing IC3_rs_oddOneOut...Done\n",
      "Pre-processing IC3_TaskRecall...Done\n",
      "Pre-processing q_IC3_demographics...Done\n",
      "Pre-processing q_IC3_fatigue...Questionnaire q_IC3_fatigue does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_GDS...Questionnaire q_IC3_GDS does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_apathy...Questionnaire q_IC3_apathy does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_IADL...Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "df= main_preprocessing(root_path, list_of_tasks, list_of_questionnaires, list_of_speech, clinical_information)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
