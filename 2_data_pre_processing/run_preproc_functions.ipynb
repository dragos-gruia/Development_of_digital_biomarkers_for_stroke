{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preproc_functions_controls import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/dg519/Documents/normative_paper_github/data'\n",
    "remote_data_folders='/data_ic3online_cognition'\n",
    "supervised_data_folders=['/data_healthy_v1','/data_healthy_v2']\n",
    "folder_structure=['/summary_data','/trial_data','/speech']\n",
    "output_clean_folder ='/data_healthy_cleaned'\n",
    "list_of_tasks = ['IC3_NVtrailMaking','IC3_Orientation', 'IC3_PearCancellation', 'IC3_rs_digitSpan', 'IC3_rs_spatialSpan', 'IC3_rs_PAL', 'IC3_rs_SRT', 'IC3_rs_CRT', 'IC3_SemanticJudgment', 'IC3_i4i_IDED', 'IC3_i4i_motorControl','IC3_calculation', 'IC3_GestureRecognition', 'IC3_AuditorySustainedAttention','IC3_BBCrs_blocks', 'IC3_Comprehension','IC3_rs_oddOneOut', 'IC3_TaskRecall']\n",
    "list_of_questionnaires = ['q_IC3_demographics', 'q_IC3_fatigue','q_IC3_GDS','q_IC3_apathy']\n",
    "list_of_speech = ['IC3_NamingTest','IC3_Reading', 'IC3_Repetition']\n",
    "folder_list = ['data_healthy_v1', 'data_healthy_v2', 'data_ic3online_cognition', 'data_healthy_merged']\n",
    "merged_data_folder ='/data_healthy_combined'\n",
    "data_format = '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_preprocessing(root_path, list_of_tasks, list_of_questionnaires, list_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics = demographics_preproc(root_path, merged_data_folder, questionnaire_name, inclusion_criteria, folder_structure, data_format,clean_file_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def general_outlier_detection_remoteSetting(root_path, remote_data_folders, folder_structure, \n",
    "                                              screening_list=None):\n",
    "    \"\"\"\n",
    "    Detect outliers in remote setting data by loading multiple screening questionnaires,\n",
    "    cleaning and merging them, and then applying exclusion criteria.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_path : str\n",
    "        The root directory.\n",
    "    remote_data_folder : str\n",
    "        Remote data folder name (e.g. '/data_ic3online_cognition').\n",
    "    folder_structure : list of str\n",
    "        List containing folder names; the first element is used to locate summary data.\n",
    "    screening_list : list of str, optional\n",
    "        List of questionnaire filenames. Defaults to:\n",
    "        ['q_IC3_demographicsHealthy_questionnaire.csv',\n",
    "         'q_IC3_metacog_questionnaire.csv',\n",
    "         'IC3_Orientation.csv',\n",
    "         'IC3_PearCancellation.csv'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        A Series of user IDs that pass all exclusion criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    if screening_list is None:\n",
    "        screening_list = [\n",
    "            'q_IC3_demographicsHealthy_questionnaire.csv',\n",
    "            'q_IC3_metacog_questionnaire.csv',\n",
    "            'IC3_Orientation.csv',\n",
    "            'IC3_PearCancellation.csv'\n",
    "        ]\n",
    "        \n",
    "    # Build the base path for the remote data\n",
    "    base_path = os.path.join(root_path, remote_data_folders.lstrip('/'), folder_structure[0].lstrip('/'))\n",
    "    \n",
    "    # Load screening questionnaires from file paths\n",
    "    df_dem    = pd.read_csv(os.path.join(base_path, screening_list[0]), low_memory=False)\n",
    "    df_cheat  = pd.read_csv(os.path.join(base_path, screening_list[1]), low_memory=False)\n",
    "    df_orient = pd.read_csv(os.path.join(base_path, screening_list[2]), low_memory=False)\n",
    "    df_pear   = pd.read_csv(os.path.join(base_path, screening_list[3]), low_memory=False)\n",
    "    \n",
    "    # Define a helper to remove duplicate and missing user IDs\n",
    "    def clean_df(df):\n",
    "        return df.drop_duplicates(subset=['user_id'], keep=\"first\").dropna(subset=['user_id']).reset_index(drop=True)\n",
    "    \n",
    "    df_dem    = clean_df(df_dem)\n",
    "    df_cheat  = clean_df(df_cheat)\n",
    "    df_orient = clean_df(df_orient)\n",
    "    df_pear   = clean_df(df_pear)\n",
    "        \n",
    "    # Keep only users present in both demographics and Pear Cancellation data\n",
    "    common_ids = set(df_dem['user_id']) ^ set(df_pear['user_id'])\n",
    "\n",
    "    for df in (df_dem, df_orient, df_pear, df_cheat):\n",
    "        df.drop(df[df['user_id'].isin(common_ids)].index, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Clean the questionnaire data\n",
    "    df_dem['Q30_R'] = df_dem['Q30_R'].replace(\"No\", \"SKIPPED\").replace(\"SKIPPED\", 999999).astype(float)\n",
    "    for col in [\"Q2_S\", \"Q3_S\"]:\n",
    "        df_cheat[col].replace([0, 1], np.nan, inplace=True)\n",
    "    df_cheat.dropna(subset=[\"Q2_S\", \"Q3_S\"], inplace=True)\n",
    "    df_cheat.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Remove users who fail screening tests based on SummaryScore criteria\n",
    "    # Vectorized approach: identify user_ids that meet both failure conditions.\n",
    "    fail_orient = set(df_orient.loc[df_orient['SummaryScore'] < 3, 'user_id'])\n",
    "    fail_pear   = set(df_pear.loc[df_pear['SummaryScore'] <= 0.80, 'user_id'])\n",
    "    failed_ids  = fail_orient & fail_pear    \n",
    "    for df in (df_dem, df_orient, df_pear):\n",
    "        df.drop(df[df['user_id'].isin(failed_ids)].index, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f'We removed {len(failed_ids)} people who failed both Orientation and Pear Cancellation, from all tasks.')\n",
    "    \n",
    "    # Remove users not neurologically healthy (using demographics responses)\n",
    "    unhealthy_mask = ((df_dem['Q12_R'] != \"SKIPPED\") | (df_dem['Q14_R'] != \"SKIPPED\") |\n",
    "                      (df_dem['Q30_R'] <= 60) | (df_dem['Q1_R'] < 40))\n",
    "    unhealthy_mask = df_dem.loc[unhealthy_mask,'user_id']\n",
    "    neuro_removed = ((df_dem['Q12_R'] != \"SKIPPED\") | (df_dem['Q14_R'] != \"SKIPPED\")).sum()\n",
    "    dementia_removed = (df_dem['Q30_R'] <= 60).sum()\n",
    "    age_removed = (df_dem['Q1_R'] < 40).sum()\n",
    "    for df in (df_dem, df_orient, df_pear):\n",
    "        df.drop(df[df['user_id'].isin(unhealthy_mask)].index, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f'We removed {neuro_removed} who indicated neurological disorder, '\n",
    "          f'{dementia_removed} with a history of dementia and {age_removed} who are younger than 40.')\n",
    "    \n",
    "    # Remove users who self-reported lack of engagement (\"cheating\")\n",
    "    cheating_mask = df_pear['user_id'].isin(df_cheat['user_id'])\n",
    "    for df in (df_dem, df_orient, df_pear):\n",
    "        df.drop(df[df['user_id'].isin(df_cheat['user_id'])].index, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f'We removed {cheating_mask.sum()} people who cheated.')\n",
    "    \n",
    "    # Return the final cleaned user IDs from df_pear as a Series\n",
    "    cleaned_ids_remote = df_pear['user_id']\n",
    "    \n",
    "    return cleaned_ids_remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 6 people who failed both Orientation and Pear Cancellation, from all tasks.\n",
      "We removed 407 who indicated neurological disorder, 91 with a history of dementia and 92 who are younger than 40.\n",
      "We removed 11 people who cheated.\n",
      "6374\n"
     ]
    }
   ],
   "source": [
    "ids2 = general_outlier_detection_remoteSetting(root_path, remote_data_folders, folder_structure)\n",
    "print(len(ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_outlier_detection_supervisedSetting(root_path, supervised_data_folders, folder_structure, screening_list = ['q_IC3_demographics_questionnaire.csv', 'IC3_Orientation.csv', 'IC3_PearCancellation.csv']): \n",
    "\n",
    "    # Read the data for v1\n",
    "\n",
    "    os.chdir(root_path + supervised_data_folders[0] + folder_structure[0])\n",
    "\n",
    "    df_dem_tp1 = pd.read_csv(screening_list[0], low_memory=False)\n",
    "    df_orient_tp1 = pd.read_csv(screening_list[1], low_memory=False)\n",
    "    df_pear_tp1 = pd.read_csv(screening_list[2], low_memory=False)\n",
    "\n",
    "    # Read the data for v2\n",
    "\n",
    "    os.chdir(root_path + supervised_data_folders[1] + folder_structure[0])\n",
    "\n",
    "    df_dem_tp2 = pd.read_csv(screening_list[0], low_memory=False)\n",
    "    df_orient_tp2 = pd.read_csv(screening_list[1], low_memory=False)\n",
    "    df_pear_tp2 = pd.read_csv(screening_list[2], low_memory=False)\n",
    "\n",
    "    # Concatenate the two timepoints\n",
    "\n",
    "    df_dem = pd.concat([df_dem_tp1, df_dem_tp2], ignore_index=True)\n",
    "    df_orient = pd.concat([df_orient_tp1, df_orient_tp2], ignore_index=True)\n",
    "    df_pear = pd.concat([df_pear_tp1, df_pear_tp2], ignore_index=True)\n",
    "\n",
    "    # Remove duplicates, NAs and reset index\n",
    "\n",
    "    df_dem.drop_duplicates(subset=['user_id'],keep=\"first\", inplace=True)\n",
    "    df_dem.dropna(subset=['user_id'], inplace=True)\n",
    "    df_dem.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_orient.drop_duplicates(subset=['user_id'],keep=\"first\", inplace=True)\n",
    "    df_orient.dropna(subset=['user_id'], inplace=True)\n",
    "    df_orient.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_pear.drop_duplicates(subset=['user_id'],keep=\"first\", inplace=True)\n",
    "    df_pear.dropna(subset=['user_id'], inplace=True)\n",
    "    df_pear.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Find the user_id who are in both df_pear and df_pear\n",
    "\n",
    "    cleaned_ids_supervised = list(set(df_dem['user_id']).intersection(set(df_orient['user_id'])).intersection(set(df_pear['user_id'])))\n",
    "\n",
    "    df_pear = df_pear[df_pear.user_id.isin(cleaned_ids_supervised)]  \n",
    "    df_orient = df_orient[df_orient.user_id.isin(cleaned_ids_supervised)]  \n",
    "    df_dem = df_dem[df_dem.user_id.isin(cleaned_ids_supervised)]  \n",
    "\n",
    "    # Drop the user_id, if they fail the screening test\n",
    "\n",
    "    ids_failed_screen = []\n",
    "    for subs in cleaned_ids_supervised:\n",
    "        if (df_orient[df_orient.user_id == subs].SummaryScore < 3).bool() and (df_pear[df_pear.user_id == subs].SummaryScore <= 0.80).bool():\n",
    "            \n",
    "            ids_failed_screen.append(subs)\n",
    "            df_dem = df_dem.drop(df_dem[df_dem.user_id == subs].index).reset_index(drop=True)\n",
    "            df_orient = df_orient.drop(df_orient[df_orient.user_id == subs].index).reset_index(drop=True)\n",
    "            df_pear = df_pear.drop(df_pear[df_pear.user_id == subs].index).reset_index(drop=True)\n",
    "\n",
    "    print(f'We removed {len(ids_failed_screen)} people who failed both Orientation and Pear Cancellation, from all tasks.')\n",
    "\n",
    "\n",
    "    cleaned_ids_supervised = df_pear.user_id\n",
    "\n",
    "    # Remove people who are not neurologically healthy\n",
    "\n",
    "    to_remove = (df_dem.Q12_R != \"SKIPPED\") | (df_dem.Q14_R != \"SKIPPED\") | (df_dem.Q1_R < 40)    \n",
    "    print(f'We removed {sum((df_dem.Q12_R != \"SKIPPED\") | (df_dem.Q14_R != \"SKIPPED\"))} who indicated they have a neurological disorder, and {sum(df_dem.Q1_R < 40)} who are younger than 40.')\n",
    "\n",
    "    df_dem = df_dem[~to_remove].reset_index(drop=True)\n",
    "    df_pear = df_pear[~to_remove].reset_index(drop=True)\n",
    "    df_orient = df_orient[~to_remove].reset_index(drop=True)\n",
    "\n",
    "    cleaned_ids_supervised = df_pear.user_id\n",
    "\n",
    "    return cleaned_ids_supervised # Return participant ids that passed exclusion criteria\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We removed 17 people who failed both Orientation and Pear Cancellation, from all tasks.\n",
      "We removed 33 who indicated they have a neurological disorder, and 4 who are younger than 40.\n",
      "338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/_3yj40_538g8lmscwcv139hm0000gp/T/ipykernel_7073/1739622952.py:51: FutureWarning: Series.bool is now deprecated and will be removed in future version of pandas\n",
      "  if (df_orient[df_orient.user_id == subs].SummaryScore < 3).bool() and (df_pear[df_pear.user_id == subs].SummaryScore <= 0.80).bool():\n"
     ]
    }
   ],
   "source": [
    "ids = general_outlier_detection_supervisedSetting(root_path, supervised_data_folders, folder_structure)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def general_outlier_detection_supervisedSetting(root_path, supervised_data_folders, folder_structure, \n",
    "                                                  screening_list=None):\n",
    "    \"\"\"\n",
    "    Detect outliers in supervised setting data by loading two timepoints of screening questionnaires,\n",
    "    cleaning and merging them, and then applying exclusion criteria.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_path : str\n",
    "        The root directory.\n",
    "    supervised_data_folders : list of str\n",
    "        List containing folder names for the two timepoints (e.g. [folder_v1, folder_v2]).\n",
    "    folder_structure : list of str\n",
    "        List containing folder names; the first element is used to locate summary data.\n",
    "    screening_list : list of str, optional\n",
    "        List of questionnaire filenames. Defaults to:\n",
    "        ['q_IC3_demographics_questionnaire.csv', 'IC3_Orientation.csv', 'IC3_PearCancellation.csv'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        A Series of user IDs that pass all exclusion criteria.\n",
    "    \"\"\"\n",
    "    if screening_list is None:\n",
    "        screening_list = [\n",
    "            'q_IC3_demographics_questionnaire.csv',\n",
    "            'IC3_Orientation.csv',\n",
    "            'IC3_PearCancellation.csv'\n",
    "        ]\n",
    "    \n",
    "    def load_timepoint_data(data_folder):\n",
    "        \"\"\"Helper function to load data for a given timepoint.\"\"\"\n",
    "        base_path = os.path.join(root_path, data_folder.lstrip('/'), folder_structure[0].lstrip('/'))\n",
    "        df_dem    = pd.read_csv(os.path.join(base_path, screening_list[0]), low_memory=False)\n",
    "        df_orient = pd.read_csv(os.path.join(base_path, screening_list[1]), low_memory=False)\n",
    "        df_pear   = pd.read_csv(os.path.join(base_path, screening_list[2]), low_memory=False)\n",
    "        return df_dem, df_orient, df_pear\n",
    "    \n",
    "    # Load data for both timepoints\n",
    "    df_dem_tp1, df_orient_tp1, df_pear_tp1 = load_timepoint_data(supervised_data_folders[0])\n",
    "    df_dem_tp2, df_orient_tp2, df_pear_tp2 = load_timepoint_data(supervised_data_folders[1])\n",
    "    \n",
    "    # Concatenate the two timepoints\n",
    "    df_dem    = pd.concat([df_dem_tp1, df_dem_tp2], ignore_index=True)\n",
    "    df_orient = pd.concat([df_orient_tp1, df_orient_tp2], ignore_index=True)\n",
    "    df_pear   = pd.concat([df_pear_tp1, df_pear_tp2], ignore_index=True)\n",
    "    \n",
    "    def clean_df(df):\n",
    "        \"\"\"Remove duplicates and missing user IDs, then reset index.\"\"\"\n",
    "        return df.drop_duplicates(subset=['user_id'], keep='first') \\\n",
    "                 .dropna(subset=['user_id']) \\\n",
    "                 .reset_index(drop=True)\n",
    "    \n",
    "    # Clean dataframes\n",
    "    df_dem    = clean_df(df_dem)\n",
    "    df_orient = clean_df(df_orient)\n",
    "    df_pear   = clean_df(df_pear)\n",
    "    \n",
    "    # Retain only common user IDs across all dataframes\n",
    "    common_ids = set(df_dem['user_id']) ^ set(df_pear['user_id'])\n",
    "    \n",
    "    for df in (df_dem, df_orient, df_pear):\n",
    "        df.drop(df[df['user_id'].isin(common_ids)].index, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Remove users who fail screening tests based on SummaryScore criteria\n",
    "    # Vectorized approach: identify user_ids that meet both failure conditions.\n",
    "    fail_orient = set(df_orient.loc[df_orient['SummaryScore'] < 3, 'user_id'])\n",
    "    fail_pear   = set(df_pear.loc[df_pear['SummaryScore'] <= 0.80, 'user_id'])\n",
    "    failed_ids  = fail_orient & fail_pear    \n",
    "    \n",
    "    for df in (df_dem, df_orient, df_pear):\n",
    "        df.drop(df[df['user_id'].isin(failed_ids)].index, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f'We removed {len(failed_ids)} people who failed both Orientation and Pear Cancellation, from all tasks.')\n",
    "\n",
    "    # Remove users not neurologically healthy (using demographics responses)\n",
    "    unhealthy_mask = ((df_dem['Q12_R'] != \"SKIPPED\") | (df_dem['Q14_R'] != \"SKIPPED\") |\n",
    "                    (df_dem['Q1_R'] < 40))\n",
    "    unhealthy_mask = df_dem.loc[unhealthy_mask,'user_id']\n",
    "    neuro_removed = ((df_dem['Q12_R'] != \"SKIPPED\") | (df_dem['Q14_R'] != \"SKIPPED\")).sum()\n",
    "    age_removed = (df_dem['Q1_R'] < 40).sum()\n",
    "    \n",
    "    for df in (df_dem, df_orient, df_pear):\n",
    "        df.drop(df[df['user_id'].isin(unhealthy_mask)].index, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f'We removed {neuro_removed} who indicated neurological disorder, '\n",
    "          f'{age_removed} who are younger than 40.')\n",
    "    \n",
    "    # Return the final cleaned user IDs from df_pear as a Series\n",
    "    cleaned_ids_supervised = df_pear['user_id']\n",
    "    \n",
    "    return cleaned_ids_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407 409 407\n",
      "392 392 392\n",
      "We removed 17 people who failed both Orientation and Pear Cancellation, from all tasks.\n",
      "375 375 375\n",
      "We removed 33 who indicated neurological disorder, 4 who are younger than 40.\n",
      "338 338 338\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "ids = general_outlier_detection_supervisedSetting(root_path, supervised_data_folders, folder_structure)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_control_data_across_sites(root_path, folder_structure, supervised_data_folders, remote_data_folders, list_of_tasks,list_of_questionnaires,list_of_speech, data_format, merged_data_folder):\n",
    "        \n",
    "    os.chdir(root_path)\n",
    "\n",
    "    # Create folder structure\n",
    "    \n",
    "    if not os.path.isdir(merged_data_folder[1:]):\n",
    "        os.mkdir(merged_data_folder[1:])\n",
    "    os.chdir(merged_data_folder[1:])\n",
    "\n",
    "    if not os.path.isdir(folder_structure[0][1:]):\n",
    "        os.mkdir(folder_structure[0][1:])\n",
    "        \n",
    "    if not os.path.isdir(folder_structure[1][1:]):\n",
    "        os.mkdir(folder_structure[1][1:])\n",
    "    \n",
    "    if not os.path.isdir(folder_structure[2][1:]):\n",
    "        os.mkdir(folder_structure[2][1:])\n",
    "\n",
    "    # Merge data from clinical tests\n",
    "    \n",
    "    if 'IC3_NVtrailMaking' in list_of_tasks:\n",
    "        list_of_tasks.append('IC3_NVtrailMaking2')\n",
    "        list_of_tasks.append('IC3_NVtrailMaking3')\n",
    "\n",
    "    for taskName in list_of_tasks:\n",
    "        \n",
    "        summary_task_path = root_path + supervised_data_folders[0] + folder_structure[0]\n",
    "        raw_task_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "        \n",
    "        df_v1 = pd.read_csv(f'{summary_task_path}/{taskName}{data_format}', low_memory=False)\n",
    "        df_v1_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        summary_task_path = root_path + supervised_data_folders[1] + folder_structure[0]\n",
    "        raw_task_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    "        \n",
    "        # Special case for IDED task that has two versions\n",
    "        \n",
    "        if taskName == \"IC3_i4i_IDED\": \n",
    "            df_v2 = pd.read_csv((f'{summary_task_path}/{taskName}2{data_format}'), low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}2_raw{data_format}'), low_memory=False)\n",
    "        else:   \n",
    "            df_v2 = pd.read_csv(f'{summary_task_path}/{taskName}{data_format}', low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        \n",
    "        summary_task_path = root_path + remote_data_folders + folder_structure[0]\n",
    "        raw_task_path = root_path + remote_data_folders + folder_structure[1]\n",
    "\n",
    "        df_cog = pd.read_csv(f'{summary_task_path}/{taskName}{data_format}', low_memory=False)\n",
    "        df_cog_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        df = pd.concat([df_v1, df_v2, df_cog], ignore_index=True)\n",
    "        df_raw = pd.concat([df_v1_raw, df_v2_raw, df_cog_raw], ignore_index=True)\n",
    "        \n",
    "        output_folder = root_path + merged_data_folder\n",
    "        \n",
    "        df.to_csv(f'{output_folder}/{folder_structure[0]}/{taskName}{data_format}', index=False)\n",
    "        df_raw.to_csv(f'{output_folder}/{folder_structure[1]}/{taskName}_raw{data_format}', index=False)\n",
    "        print(f'Merged {taskName}')\n",
    "        print(len(df))\n",
    "        \n",
    "    if 'IC3_NVtrailMaking' in list_of_tasks:\n",
    "        list_of_tasks = list_of_tasks[:-2]\n",
    "        \n",
    "    # Merge data from speech\n",
    "        \n",
    "    for taskName in list_of_speech:\n",
    "        \n",
    "        raw_speech_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "        df_v1_raw = pd.read_csv((f'{raw_speech_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "        \n",
    "        raw_speech_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    "        df_v2_raw = pd.read_csv((f'{raw_speech_path}/{taskName}_raw{data_format}'), low_memory=False)\n",
    "\n",
    "        df_raw = pd.concat([df_v1_raw, df_v2_raw], ignore_index=True)\n",
    "        \n",
    "        output_folder = root_path + merged_data_folder\n",
    "        df_raw.to_csv(f'{output_folder}/{folder_structure[1]}/{taskName}_raw.csv', index=False)\n",
    "        print(f'Merged {taskName}')\n",
    "        print(len(df_raw))\n",
    "        \n",
    "    # Merge data from questionnaires\n",
    "\n",
    "    for taskName in list_of_questionnaires:\n",
    "        \n",
    "        if taskName == 'q_IC3_demographics':\n",
    "            \n",
    "            summary_task_path = root_path + supervised_data_folders[0] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "            \n",
    "            df_v1 = pd.read_csv((f'{summary_task_path}/{taskName}Healthy_questionnaire.csv'), low_memory=False)\n",
    "            df_v1_2 = pd.read_csv((f'{summary_task_path}/{taskName}_questionnaire.csv'), low_memory=False)\n",
    "            df_v1_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            df_v1_raw_2 = pd.read_csv((f'{raw_task_path}/{taskName}Healthy_raw.csv'), low_memory=False)\n",
    "\n",
    "            summary_task_path = root_path + supervised_data_folders[1] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    " \n",
    "            df_v2 = pd.read_csv((f'{summary_task_path}/{taskName}_questionnaire.csv'), low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            summary_task_path = root_path + remote_data_folders + folder_structure[0]\n",
    "            raw_task_path = root_path + remote_data_folders + folder_structure[1]\n",
    "\n",
    "            df_cog = pd.read_csv((f'{summary_task_path}/{taskName}Healthy_questionnaire.csv'), low_memory=False)\n",
    "            df_cog_raw = pd.read_csv((f'{raw_task_path}/{taskName}Healthy_raw.csv'), low_memory=False)\n",
    "\n",
    "            df = pd.concat([df_v1, df_v1_2, df_v2, df_cog], ignore_index=True)\n",
    "            df_raw = pd.concat([df_v1_raw, df_v1_raw_2, df_v2_raw, df_cog_raw], ignore_index=True)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            summary_task_path = root_path + supervised_data_folders[0] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[0] + folder_structure[1]\n",
    "            \n",
    "            df_v1 = pd.read_csv(f'{summary_task_path}/{taskName}_questionnaire.csv', low_memory=False)\n",
    "            df_v1_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            summary_task_path = root_path + supervised_data_folders[1] + folder_structure[0]\n",
    "            raw_task_path = root_path + supervised_data_folders[1] + folder_structure[1]\n",
    "            \n",
    "            df_v2 = pd.read_csv(f'{summary_task_path}/{taskName}_questionnaire.csv', low_memory=False)\n",
    "            df_v2_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            summary_task_path = root_path + remote_data_folders + folder_structure[0]\n",
    "            raw_task_path = root_path + remote_data_folders + folder_structure[1]\n",
    "            \n",
    "            df_cog = pd.read_csv(f'{summary_task_path}/{taskName}_questionnaire.csv', low_memory=False)\n",
    "            df_cog_raw = pd.read_csv((f'{raw_task_path}/{taskName}_raw.csv'), low_memory=False)\n",
    "            \n",
    "            df = pd.concat([df_v1, df_v2, df_cog], ignore_index=True)\n",
    "            df_raw = pd.concat([df_v1_raw, df_v2_raw, df_cog_raw], ignore_index=True)\n",
    "        \n",
    "        output_folder = root_path + merged_data_folder\n",
    "\n",
    "        #df.to_csv(f'{output_folder}/{folder_structure[0]}/{taskName}{data_format}', index=False)\n",
    "        #df_raw.to_csv(f'{output_folder}/{folder_structure[1]}/{taskName}_raw.csv', index=False)\n",
    "        print(f'Merged {taskName}')\n",
    "    \n",
    "        print(len(df))\n",
    "    \n",
    "    return list_of_tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IC3_NVtrailMaking',\n",
       " 'IC3_Orientation',\n",
       " 'IC3_PearCancellation',\n",
       " 'IC3_rs_digitSpan',\n",
       " 'IC3_rs_spatialSpan',\n",
       " 'IC3_rs_PAL',\n",
       " 'IC3_rs_SRT',\n",
       " 'IC3_rs_CRT',\n",
       " 'IC3_SemanticJudgment',\n",
       " 'IC3_i4i_IDED',\n",
       " 'IC3_i4i_motorControl',\n",
       " 'IC3_calculation',\n",
       " 'IC3_GestureRecognition',\n",
       " 'IC3_AuditorySustainedAttention',\n",
       " 'IC3_BBCrs_blocks',\n",
       " 'IC3_Comprehension',\n",
       " 'IC3_rs_oddOneOut',\n",
       " 'IC3_TaskRecall',\n",
       " 'IC3_NVtrailMaking2',\n",
       " 'IC3_NVtrailMaking3']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged IC3_NVtrailMaking\n",
      "6330\n",
      "Merged IC3_Orientation\n",
      "7438\n",
      "Merged IC3_PearCancellation\n",
      "7401\n",
      "Merged IC3_rs_digitSpan\n",
      "6354\n",
      "Merged IC3_rs_spatialSpan\n",
      "6961\n",
      "Merged IC3_rs_PAL\n",
      "6763\n",
      "Merged IC3_rs_SRT\n",
      "6455\n",
      "Merged IC3_rs_CRT\n",
      "6425\n",
      "Merged IC3_SemanticJudgment\n",
      "5980\n",
      "Merged IC3_i4i_IDED\n",
      "6469\n",
      "Merged IC3_i4i_motorControl\n",
      "6401\n",
      "Merged IC3_calculation\n",
      "6656\n",
      "Merged IC3_GestureRecognition\n",
      "6273\n",
      "Merged IC3_AuditorySustainedAttention\n",
      "7143\n",
      "Merged IC3_BBCrs_blocks\n",
      "6242\n",
      "Merged IC3_Comprehension\n",
      "6034\n",
      "Merged IC3_rs_oddOneOut\n",
      "6700\n",
      "Merged IC3_TaskRecall\n",
      "5639\n",
      "Merged IC3_NVtrailMaking2\n",
      "6313\n",
      "Merged IC3_NVtrailMaking3\n",
      "6332\n",
      "Merged IC3_NamingTest\n",
      "10666\n",
      "Merged IC3_Reading\n",
      "3748\n",
      "Merged IC3_Repetition\n",
      "7028\n",
      "Merged q_IC3_demographics\n",
      "7659\n",
      "Merged q_IC3_fatigue\n",
      "5579\n",
      "Merged q_IC3_GDS\n",
      "5592\n",
      "Merged q_IC3_apathy\n",
      "5610\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "list_of_tasks = merge_control_data_across_sites(root_path, folder_structure, supervised_data_folders, remote_data_folders, list_of_tasks,list_of_questionnaires,list_of_speech, data_format, merged_data_folder)\n",
    "print(len(list_of_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def ensure_directory(path):\n",
    "    \"\"\"Checks if directory exists, creates it if it does not exist.\"\"\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def load_task_data(root, data_folder, subfolder, task, data_format, raw=False, version_suffix=''):\n",
    "    \"\"\"\n",
    "    Helper to load a CSV file.\n",
    "    :param root: Root directory.\n",
    "    :param data_folder: The data folder (e.g. supervised or remote).\n",
    "    :param subfolder: The subfolder name (e.g. summary or raw folder).\n",
    "    :param task: Task name.\n",
    "    :param data_format: File extension (e.g. '.csv').\n",
    "    :param raw: If True, load the raw file.\n",
    "    :param version_suffix: Optional suffix (for special cases like IDED).\n",
    "    \"\"\"\n",
    "    \n",
    "    base = os.path.join(root, data_folder.strip(\"/\"), subfolder.strip(\"/\"), task)\n",
    "    suffix = \"_raw\" if raw else \"\"\n",
    "    filename = f\"{base}{version_suffix}{suffix}{data_format}\"\n",
    "\n",
    "    return pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "def merge_and_save(df_list, output_file):\n",
    "    \"\"\"\n",
    "    Concatenate list of dataframes and save to a CSV file.\n",
    "    :param df_list: List of dataframes to merge.\n",
    "    :param output_file: Full path of the output file.\n",
    "    \"\"\"\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(len(merged_df))\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "def merge_clinical_tests(root, folder_structure, supervised_data_folders, remote_data_folder, tasks, data_format, output_base):\n",
    "    \n",
    "    \"\"\"\n",
    "    Merge clinical test data across sources.\n",
    "    This function handles the special case for 'IC3_i4i_IDED' and temporary addition of NVtrailMaking versions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use a copy of the tasks list to add temporary tasks\n",
    "    tasks_to_merge = tasks.copy()\n",
    "    if 'IC3_NVtrailMaking' in tasks_to_merge:\n",
    "        tasks_to_merge += ['IC3_NVtrailMaking2', 'IC3_NVtrailMaking3']\n",
    "\n",
    "    for task in tasks_to_merge:\n",
    "        # Supervised source 1\n",
    "        df_v1 = load_task_data(root, supervised_data_folders[0], folder_structure[0], task, data_format, raw=False)\n",
    "        df_v1_raw = load_task_data(root, supervised_data_folders[0], folder_structure[1], task, data_format, raw=True)\n",
    "        \n",
    "        # Supervised source 2 (with special case for IDED)\n",
    "        if task == \"IC3_i4i_IDED\":\n",
    "            df_v2 = load_task_data(root, supervised_data_folders[1], folder_structure[0], task, data_format, raw=False, version_suffix='2')\n",
    "            df_v2_raw = load_task_data(root, supervised_data_folders[1], folder_structure[1], task, data_format, raw=True, version_suffix='2')\n",
    "        else:\n",
    "            df_v2 = load_task_data(root, supervised_data_folders[1], folder_structure[0], task, data_format, raw=False)\n",
    "            df_v2_raw = load_task_data(root, supervised_data_folders[1], folder_structure[1], task, data_format, raw=True)\n",
    "        \n",
    "        # Remote data source\n",
    "        df_cog = load_task_data(root, remote_data_folder, folder_structure[0], task, data_format, raw=False)\n",
    "        df_cog_raw = load_task_data(root, remote_data_folder, folder_structure[1], task, data_format, raw=True)\n",
    "        \n",
    "        print(task)\n",
    "        \n",
    "        # Merge the three sources\n",
    "        summary_out = os.path.join(output_base, folder_structure[0].strip(\"/\"), f\"{task}{data_format}\")\n",
    "        merge_and_save([df_v1, df_v2, df_cog], summary_out)\n",
    "        \n",
    "        raw_out = os.path.join(output_base, folder_structure[1].strip(\"/\"), f\"{task}_raw{data_format}\")\n",
    "        merge_and_save([df_v1_raw, df_v2_raw, df_cog_raw],raw_out)\n",
    "\n",
    "        # Optionally log progress: print(f'Merged clinical test data for {task}')\n",
    "\n",
    "def merge_speech_data(root, folder_structure, supervised_data_folders, list_of_speech,  data_format, output_base):\n",
    "    \"\"\"\n",
    "    Merge speech data from two supervised sources.\n",
    "    \"\"\"\n",
    "    for task in list_of_speech:\n",
    "        print(task)\n",
    "        \n",
    "        df_v1_raw = load_task_data(root, supervised_data_folders[0], folder_structure[1].strip(\"/\"), task, data_format, raw=True)\n",
    "        df_v2_raw = load_task_data(root, supervised_data_folders[1], folder_structure[1].strip(\"/\"), task, data_format, raw=True)\n",
    "        \n",
    "        raw_out = os.path.join(output_base, folder_structure[1].strip(\"/\"), f\"{task}_raw{data_format}\")\n",
    "        merge_and_save([df_v1_raw, df_v2_raw], raw_out)\n",
    "        \n",
    "        \n",
    "        # Optionally log progress: print(f'Merged speech data for {task}')\n",
    "\n",
    "def merge_questionnaire_data(root, folder_structure, supervised_data_folders, remote_data_folder, list_of_questionnaires,  data_format, output_base):\n",
    "    \"\"\"\n",
    "    Merge questionnaire data. Handles the special case for demographics.\n",
    "    \"\"\"\n",
    "    for task in list_of_questionnaires:\n",
    "        \n",
    "        print(task)\n",
    "\n",
    "        if task == 'q_IC3_demographics':\n",
    "            \n",
    "            # Supervised source 1 (two versions for healthy)\n",
    "            df_v1 = load_task_data(root, supervised_data_folders[0], folder_structure[0].strip(\"/\"), task, data_format, raw=False, version_suffix='Healthy_questionnaire')\n",
    "            df_v1_2 = load_task_data(root, supervised_data_folders[0], folder_structure[0].strip(\"/\"), task, data_format, raw=False, version_suffix='_questionnaire')\n",
    "            df_v1_raw = load_task_data(root, supervised_data_folders[0], folder_structure[1].strip(\"/\"), task, data_format, raw=True)\n",
    "            df_v1_raw_2 = load_task_data(root, supervised_data_folders[0], folder_structure[1].strip(\"/\"), task, data_format, raw=True, version_suffix='Healthy')\n",
    "\n",
    "            # Supervised source 2\n",
    "            df_v2 = load_task_data(root, supervised_data_folders[1], folder_structure[0].strip(\"/\"), task, data_format, raw=False, version_suffix='_questionnaire')\n",
    "            df_v2_raw = load_task_data(root, supervised_data_folders[1], folder_structure[1].strip(\"/\"), task, data_format, raw=True)\n",
    "            \n",
    "            # Remote data source\n",
    "            df_cog =  load_task_data(root, remote_data_folder, folder_structure[0].strip(\"/\"), task, data_format, raw=False, version_suffix='Healthy_questionnaire')\n",
    "            df_cog_raw =  load_task_data(root, remote_data_folder, folder_structure[1].strip(\"/\"), task, data_format, raw=True, version_suffix='Healthy')\n",
    "\n",
    "            summary_out = os.path.join(output_base, folder_structure[0].strip(\"/\"), f\"{task}{data_format}\")\n",
    "            merge_and_save([df_v1, df_v1_2, df_v2, df_cog], summary_out)\n",
    "            \n",
    "            raw_out = os.path.join(output_base, folder_structure[1].strip(\"/\"), f\"{task}_raw{data_format}\")\n",
    "            merge_and_save([df_v1_raw, df_v1_raw_2, df_v2_raw, df_cog_raw], raw_out)\n",
    "\n",
    "        else:\n",
    "            # For all other questionnaires\n",
    "            # Supervised source 1\n",
    "\n",
    "            df_v1 = load_task_data(root, supervised_data_folders[0], folder_structure[0].strip(\"/\"), task, data_format, raw=False, version_suffix='_questionnaire')\n",
    "            df_v1_raw = load_task_data(root, supervised_data_folders[0], folder_structure[1].strip(\"/\"), task, data_format, raw=True)\n",
    "\n",
    "            # Supervised source 2\n",
    "            df_v2 = load_task_data(root, supervised_data_folders[1], folder_structure[0].strip(\"/\"), task, data_format, raw=False, version_suffix='_questionnaire')\n",
    "            df_v2_raw = load_task_data(root, supervised_data_folders[1], folder_structure[1].strip(\"/\"), task, data_format, raw=True)\n",
    "            \n",
    "            # Remote data source\n",
    "            df_cog =  load_task_data(root, remote_data_folder, folder_structure[0].strip(\"/\"), task, data_format, raw=False, version_suffix='_questionnaire')\n",
    "            df_cog_raw =  load_task_data(root, remote_data_folder, folder_structure[1].strip(\"/\"), task, data_format, raw=True)\n",
    "            \n",
    "            summary_out = os.path.join(output_base, folder_structure[0].strip(\"/\"), f\"{task}{data_format}\")\n",
    "            merge_and_save([df_v1, df_v2, df_cog], summary_out)\n",
    "            \n",
    "            raw_out = os.path.join(output_base, folder_structure[1].strip(\"/\"), f\"{task}_raw{data_format}\")\n",
    "            merge_and_save([df_v1_raw, df_v2_raw, df_cog_raw], raw_out)\n",
    "            \n",
    "        # Optionally log progress: print(f'Merged questionnaire data for {task}')\n",
    "\n",
    "def merge_control_data_across_sites(root_path, folder_structure, supervised_data_folders,\n",
    "                                    remote_data_folders, list_of_tasks, list_of_questionnaires,\n",
    "                                    list_of_speech, data_format, merged_data_folder):\n",
    "    \"\"\"\n",
    "    Merge control data across sites from clinical tests, speech, and questionnaires.\n",
    "    \"\"\"\n",
    "    # Build output base folder and ensure directory structure exists\n",
    "    output_base = os.path.join(root_path, merged_data_folder.strip(\"/\"))\n",
    "    ensure_directory(output_base)\n",
    "    \n",
    "    for folder in folder_structure:\n",
    "        ensure_directory(os.path.join(output_base, folder.strip(\"/\")))\n",
    "    \n",
    "    # Merge clinical test data\n",
    "    merge_clinical_tests(root_path, folder_structure, supervised_data_folders,\n",
    "                                         remote_data_folders, list_of_tasks, data_format, output_base)\n",
    "    \n",
    "    # Merge speech data\n",
    "    merge_speech_data(root_path, folder_structure, supervised_data_folders, list_of_speech,  data_format, output_base)\n",
    "    \n",
    "    # Merge questionnaire data\n",
    "    merge_questionnaire_data(root_path, folder_structure, supervised_data_folders, remote_data_folders,\n",
    "                             list_of_questionnaires,  data_format, output_base)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IC3_NVtrailMaking',\n",
       " 'IC3_Orientation',\n",
       " 'IC3_PearCancellation',\n",
       " 'IC3_rs_digitSpan',\n",
       " 'IC3_rs_spatialSpan',\n",
       " 'IC3_rs_PAL',\n",
       " 'IC3_rs_SRT',\n",
       " 'IC3_rs_CRT',\n",
       " 'IC3_SemanticJudgment',\n",
       " 'IC3_i4i_IDED',\n",
       " 'IC3_i4i_motorControl',\n",
       " 'IC3_calculation',\n",
       " 'IC3_GestureRecognition',\n",
       " 'IC3_AuditorySustainedAttention',\n",
       " 'IC3_BBCrs_blocks',\n",
       " 'IC3_Comprehension',\n",
       " 'IC3_rs_oddOneOut',\n",
       " 'IC3_TaskRecall']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC3_NVtrailMaking\n",
      "6330\n",
      "93565\n",
      "IC3_Orientation\n",
      "7438\n",
      "29727\n",
      "IC3_PearCancellation\n",
      "7401\n",
      "168409\n",
      "IC3_rs_digitSpan\n",
      "6354\n",
      "55975\n",
      "IC3_rs_spatialSpan\n",
      "6961\n",
      "48779\n",
      "IC3_rs_PAL\n",
      "6763\n",
      "165256\n",
      "IC3_rs_SRT\n",
      "6455\n",
      "394523\n",
      "IC3_rs_CRT\n",
      "6425\n",
      "405563\n",
      "IC3_SemanticJudgment\n",
      "5980\n",
      "137628\n",
      "IC3_i4i_IDED\n",
      "6469\n",
      "378462\n",
      "IC3_i4i_motorControl\n",
      "6401\n",
      "191790\n",
      "IC3_calculation\n",
      "6656\n",
      "58849\n",
      "IC3_GestureRecognition\n",
      "6273\n",
      "50050\n",
      "IC3_AuditorySustainedAttention\n",
      "7143\n",
      "252867\n",
      "IC3_BBCrs_blocks\n",
      "6242\n",
      "158113\n",
      "IC3_Comprehension\n",
      "6034\n",
      "119656\n",
      "IC3_rs_oddOneOut\n",
      "6700\n",
      "93002\n",
      "IC3_TaskRecall\n",
      "5639\n",
      "22524\n",
      "IC3_NVtrailMaking2\n",
      "6313\n",
      "89333\n",
      "IC3_NVtrailMaking3\n",
      "6332\n",
      "92736\n",
      "IC3_NamingTest\n",
      "10666\n",
      "IC3_Reading\n",
      "3748\n",
      "IC3_Repetition\n",
      "7028\n",
      "q_IC3_demographics\n",
      "7659\n",
      "536548\n",
      "q_IC3_fatigue\n",
      "5579\n",
      "55860\n",
      "q_IC3_GDS\n",
      "5592\n",
      "179222\n",
      "q_IC3_apathy\n",
      "5610\n",
      "213784\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "merge_control_data_across_sites(root_path, folder_structure, supervised_data_folders,\n",
    "                                    remote_data_folders, list_of_tasks, list_of_questionnaires,\n",
    "                                    list_of_speech, data_format, merged_data_folder)\n",
    "print(len(list_of_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preproc_functions_patients import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/dg519/Documents/normative_paper_github/data'\n",
    "list_of_tasks = ['IC3_Orientation', 'IC3_PearCancellation', 'IC3_rs_digitSpan', 'IC3_rs_spatialSpan', 'IC3_rs_PAL', 'IC3_rs_SRT', 'IC3_rs_CRT', 'IC3_SemanticJudgment', 'IC3_i4i_IDED', 'IC3_i4i_motorControl','IC3_calculation', 'IC3_GestureRecognition', 'IC3_AuditorySustainedAttention','IC3_BBCrs_blocks', 'IC3_Comprehension', 'IC3_NVtrailMaking','IC3_rs_oddOneOut', 'IC3_TaskRecall']\n",
    "list_of_questionnaires = ['q_IC3_demographics', 'q_IC3_fatigue','q_IC3_GDS','q_IC3_apathy', 'q_IC3_IADL']\n",
    "list_of_speech = ['IC3_NamingTest','IC3_Reading', 'IC3_Repetition']\n",
    "patient_data_folders=['/data_patients_v1','/data_patients_v2']\n",
    "folder_structure=['/summary_data','/trial_data','/speech']\n",
    "output_clean_folder ='/data_patients_cleaned'\n",
    "merged_data_folder ='/data_patients_merged'\n",
    "clean_file_extension='_cleaned'\n",
    "data_format='.csv'\n",
    "clinical_information = '/Users/dg519/Documents/normative_paper_github/data/output_data/patients_database260924.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Merging data across sites...Merged IC3_Orientation\n",
      "Merged IC3_PearCancellation\n",
      "Merged IC3_rs_digitSpan\n",
      "Merged IC3_rs_spatialSpan\n",
      "Merged IC3_rs_PAL\n",
      "Merged IC3_rs_SRT\n",
      "Merged IC3_rs_CRT\n",
      "Merged IC3_SemanticJudgment\n",
      "Merged IC3_i4i_IDED\n",
      "Merged IC3_i4i_motorControl\n",
      "Merged IC3_calculation\n",
      "Merged IC3_GestureRecognition\n",
      "Merged IC3_AuditorySustainedAttention\n",
      "Merged IC3_BBCrs_blocks\n",
      "Merged IC3_Comprehension\n",
      "Merged IC3_NVtrailMaking\n",
      "Merged IC3_rs_oddOneOut\n",
      "Merged IC3_TaskRecall\n",
      "Merged IC3_NamingTest\n",
      "Merged IC3_Reading\n",
      "Merged IC3_Repetition\n",
      "Merged q_IC3_demographics\n",
      "Merged q_IC3_fatigue\n",
      "Merged q_IC3_GDS\n",
      "Merged q_IC3_apathy\n",
      "Merged q_IC3_IADL\n",
      "Done\n",
      "Pre-processing IC3_Orientation...Done\n",
      "Pre-processing IC3_PearCancellation...Done\n",
      "Pre-processing IC3_rs_digitSpan...Done\n",
      "Pre-processing IC3_rs_spatialSpan...Done\n",
      "Pre-processing IC3_rs_PAL...Done\n",
      "Pre-processing IC3_rs_SRT...Done\n",
      "Pre-processing IC3_rs_CRT...Done\n",
      "Pre-processing IC3_SemanticJudgment...Done\n",
      "Pre-processing IC3_i4i_IDED...Done\n",
      "Pre-processing IC3_i4i_motorControl...Done\n",
      "Pre-processing IC3_calculation...Done\n",
      "Pre-processing IC3_GestureRecognition...Done\n",
      "Pre-processing IC3_AuditorySustainedAttention...Done\n",
      "Pre-processing IC3_BBCrs_blocks...Done\n",
      "Pre-processing IC3_Comprehension...Done\n",
      "Pre-processing IC3_NVtrailMaking...Done\n",
      "Pre-processing IC3_rs_oddOneOut...Done\n",
      "Pre-processing IC3_TaskRecall...Done\n",
      "Pre-processing q_IC3_demographics...Done\n",
      "Pre-processing q_IC3_fatigue...Questionnaire q_IC3_fatigue does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_GDS...Questionnaire q_IC3_GDS does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_apathy...Questionnaire q_IC3_apathy does not have a specific preprocessing function.\n",
      "Pre-processing q_IC3_IADL...Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "df= main_preprocessing(root_path, list_of_tasks, list_of_questionnaires, list_of_speech, clinical_information)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
